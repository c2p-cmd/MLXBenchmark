# Literature Review

## A. Supervised Churn Prediction: State-of-the-Art

**Common High-Accuracy Algorithms:** Supervised learning is commonly used to separate churners from non-churners.

1. **Random Forest (RF)** is a popular ensemble classifier for churn prediction.
2. **Gradient Boosting** (specifically **XGBoost**) and heterogeneous ensemble classifiers frequently demonstrate **superior performance** (higher AUC) compared to single models like Logistic Regression (LR) and RF for predicting churn.

- In one telecommunications benchmark, **XGBoost achieved an AUC of 0.95** for churn prediction.
- In banking, RF achieved high accuracy, around 97%. **Deep Learning (DL)** architectures, such as deep feed-forward networks, are applied to churn prediction, motivated by the goal of achieving high accuracy while eliminating manual feature engineering by learning abstract features in an unsupervised manner.
- In comparison against a proprietary Random Forest model, a proposed deep learning pipeline demonstrated **superior prediction accuracy** (up to 15% increase) for certain companies.

## B. Unsupervised Segmentation in Finance/E-commerce

**Algorithms Used:** Clustering is a key unsupervised technique for discovering unknown, valuable behavioral patterns.
**K-means** is the most popular and frequently used technique for customer segmentation in both telecommunication and e-commerce industries. Other clustering methods include Fuzzy c-means (FCM), DBSCAN (Density-based Spatial Clustering of Applications with Noise), Hierarchical clustering (like BIRCH), Gaussian Mixture Model (GMM), Latent Class Analysis (LCA), and Self-Organizing Maps (SOM).

**Data and Features:** Segmentation relies on customer data such as behavioral characteristics, online behavior, demographic, psychographic, and geographic inputs.

In e-commerce, the **RFM (Recency, Frequency, Monetary)** model is commonly used to segment customers. In finance, studies use K-means clustering to segment customers based on credit card behavior data.

**Validation Metrics:** Clustering is frequently evaluated using the **Silhouette Coefficient**, which measures how similar an object is to its own cluster versus others. The Elbow Method is often utilized to determine the optimal number of clusters (_k_). Domain knowledge may also suggest the number of clusters (e.g., 7 homogeneous groups).

## C. Methodological Gap: Unsupervised-to-Supervised Evaluation

**Hybridization Methods:** Hybrid models combining unsupervised clustering (Stage 1) and supervised classification (Stage 2) are used to improve predictive performance.

Two main methods are explored:

1. **Building a separate classifier for each homogeneous cluster/segment**.
2. **Using the cluster labels/identity as an input feature** to a single supervised model.

**Comparative Results:** For flexible, tree-based models (RF, XGBoost, Ensemble), the **benchmark approach (no clustering)** consistently performs the best (highest AUC for churn), suggesting these models automatically identify better splits than explicit clustering methods. However, for less flexible models like **Logistic Regression (LR)**, the hybrid approach (building an LR model per cluster/segment) results in a **higher AUC for churn prediction** than the benchmark LR model. This suggests that fitting an LR model per cluster better captures the complex relations in the data. In the two methods of hybridization, including the cluster label as an input feature to the decision tree (Method 2) was generally found to be a better method than training separate models per cluster (Method 1). **Segmentation Efficacy:** One study concluded that customer segmentation **does not have a significant impact** on churn prediction precision, as the results depend heavily on the specific dataset and model choice.

**Metrics for Comparison:** The **Adjusted Rand Index (ARI)** is mentioned as a metric to measure the similarity between two different clusterings of the same dataset (e.g., comparing clusterings of a full dataset vs. a subset). The final measure of business value is achieved by comparing the **AUC** of the hybrid model against the benchmark AUC.

## D. Computational/Framework Benchmarking (MLX Gap)

- **Apple Silicon Architecture:** Apple Silicon (M1/M2 series) uses a **Unified Memory** architecture that integrates CPU and GPU memory, offering a shared pool of up to 192GB. This contrasts with the VRAM limits of traditional NVIDIA GPUs.
- **Frameworks:** **PyTorch** supports Apple Silicon via the **Metal Performance Shaders (MPS) backend**.
- **MLX** is an array framework optimized for Apple Silicon, leveraging its unified memory to eliminate data transfer overhead.
- **Performance Gaps:** Apple Silicon generally underperforms NVIDIA GPUs in LLM training, especially when the required memory footprint is smaller than the dedicated GPU capacity. This performance gap is attributed to system factors like **page faults** and inferior performance in basic linear algebra subprograms (BLAS) compared to optimized NVIDIA kernels.
- **MLX Performance:** MLX kernels generally achieve a speed advantage over PyTorch MPS. However, even optimized kernels only achieve a 30%–40% speed advantage over PyTorch. **GenAI/LLM Focus:** Benchmarking frameworks like **CONSUMERBENCH** and studies using **MLX-LM** evaluate Generative AI models (LLMs like Llama, Mistral, Whisper) on Apple Silicon under concurrency. These studies conclude that small, 4-bit quantized models (1B–1.5B parameters) running on MLX-LM strike a **favorable balance** of responsiveness and memory efficiency for local deployment.
- **Absence of Classic ML Benchmarks:** The benchmarking studies on Apple Silicon (MLX/MPS) primarily focus on **Deep Learning and Transformer models** (LLMs). There is no explicit mention or benchmarking of traditional clustering algorithms such as K-Means or DBSCAN using the specialized MLX framework or PyTorch/scikit-learn environment for Apple Silicon in these sources.

## Summary Table

The papers are grouped below based on their relevance to the methodological components of Clustering/Segmentation, Hybrid Models/Churn, and MLX/Benchmarking.

### Papers Relevant to MLX Framework and Benchmarking (Objective 4 & 5)

| Paper Title / Context                                                                     | Authors                                                                | Keywords / Relevance to Project                                                                                                                                                                                                                                                            |
| :---------------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Comparative Study of Deep Learning Software Frameworks**                                | Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, Mohak Shah       | Caffe, Neon, TensorFlow, Theano, **Torch**, comparative study, speed, hardware utilization. _(Relevance: Provides methodology and baseline comparative performance of PyTorch/Torch against other established DL frameworks.)_                                                             |
| **Profiling Apple Silicon Performance for ML Training**                                   | Dahua Feng, Zhiming Xu, Rongxiang Wang, Felix Xiaozhu Lin              | **Apple Silicon, Unified Memory, LLM Training, BLAS kernel analysis**. _(Relevance: Directly addresses the performance gap between Apple Silicon (using MPS and MLX) and NVIDIA GPUs, detailing memory management and BLAS operations crucial for low-level benchmarking.)_                |
| **MLX: Efficient and flexible machine learning on Apple Silicon** (Cited as reference in) | Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert | **MLX**, Efficient and flexible machine learning on **Apple Silicon**. _(Relevance: The foundational paper introducing the MLX framework, essential for justifying its selection in Objective 4.)_                                                                                         |
| **CONSUMERBENCH: Benchmarking Generative AI Applications on End-User Devices**            | Yile Gu, Rohan Kadekodi, Hoang Nguyen, et al.                          | **Generative AI, End-User Devices, SLO, GPU Utilization**. _(Relevance: Benchmarks ML applications on consumer-grade hardware (MacBook M1 Pro) under concurrent loads and mentions using the **MLX framework** to accelerate specific workloads (Whisper-Large-v3-turbo implementation).)_ |
| **Evaluating Small Quantized Language Models on Apple Silicon**                           | Oleh Chaplia, Halyna Klym                                              | small language models, **Apple Silicon, MLX, MLX-LM**. _(Relevance: Directly benchmarks ML performance on Apple Silicon using MLX-LM for tasks related to efficiency and local deployment, validating the platform's utility for resource-constrained tasks.)_                             |
| **Scikit-learn: Machine Learning in Python**                                              | F. Pedregosa, G. Varoquaux, A. Gramfort, et al.                        | **scikit-learn**, Machine Learning in Python. _(Relevance: The primary reference for the established framework used as a performance comparison baseline in your project.)_                                                                                                                |

### Papers Relevant to Clustering, Segmentation, and Credit Card Data (Objective 1 & 2)

| Paper Title / Context                                                                                            | Authors                                          | Keywords / Relevance to Project                                                                                                                                                                                                                                           |
| :--------------------------------------------------------------------------------------------------------------- | :----------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **The Investigation of Machine Learning Approaches for Customer Segmentation**                                   | Ziying Chen                                      | **Customer Segmentation, Machine Learning, K-Means**, Artificial Intelligence. _(Relevance: Comprehensive review covering K-Means, DBSCAN, and other clustering methods for segmentation, establishing state-of-the-art for unsupervised methods.)_                       |
| **Customer churn prediction in the banking sector using machine learning-based classification models**           | Hoang Tran, Ngoc Le, Van-Ho Nguyen               | **churn prediction, machine learning, banking industry, classification models**. _(Relevance: Explicitly uses **K-means clustering to segment customers** in the **banking sector** and evaluates its impact on churn prediction using supervised models (LR, RF, SVM).)_ |
| **Transforming Customer Segmentation with Unsupervised Learning Models and Behavioral Data in Digital Commerce** | Ishola Bayo Ridwan                               | **Unsupervised learning, Customer segmentation, Behavioral analytics, Clustering models**. _(Relevance: Highlights the use of clustering algorithms like **K-means** and **DBSCAN** for segmenting customers based on **behavioral data**.)_                              |
| **B2C E-commerce customer churn prediction based on k-means and SVM**                                            | X. Xiahou & Y. Harada                            | **k-means and SVM, E-commerce customer churn prediction**. _(Relevance: Combines K-means clustering for segmentation with SVM for prediction, supporting the concept of combining unsupervised and supervised methods.)_                                                  |
| **Customer Segmentation and Profiling For E-Commerce Using Dbscan And Fuzzy C-Means**                            | N. Snehalatha, et al.                            | DBSCAN, Fuzzy c-means. _(Relevance: Utilizes **DBSCAN** for customer segmentation, validating the use of density-based clustering algorithms on real-world datasets.)_                                                                                                    |
| **Research on E-commerce Customer Segmentation Based on the K-means++ algorithm**                                | Z. Jing                                          | K-means++ algorithm. _(Relevance: Discusses K-means variants like K-means++ used for segmentation.)_                                                                                                                                                                      |
| **Credit card fraud detection - machine learning methods**                                                       | D. Varmedja, M. Karanovic, S. Sladojevic, et al. | Credit card fraud detection. _(Relevance: Mentions the direct application of ML in the financial sector using credit card data.)_                                                                                                                                         |

### Papers Relevant to Hybrid and Supervised Churn Prediction (Objective 3)

| Paper Title / Context                                                                  | Authors                                            | Keywords / Relevance to Project                                                                                                                                                                                                                                                                         |
| :------------------------------------------------------------------------------------- | :------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Predicting Churn using Hybrid Supervised-Unsupervised Models**                       | Meeke Rijnen                                       | **Hybrid supervised-unsupervised Models, Logistic Regression, Random Forest, XGBoost**. _(Relevance: Directly compares benchmark (non-hybrid) supervised models (RF, XGBoost, LR) against hybrid models where clustering results (GMM, LCA) are utilized to improve churn prediction accuracy (AUC).)_  |
| **Hybrid Models Using Unsupervised Clustering for Prediction of Customer Churn**       | Indranil Bose and Xi Chen                          | **Churn, Clustering, Data mining, Decision trees, Prediction**. _(Relevance: Investigates combining unsupervised clustering (K-means, SOM, BIRCH, FCM) with supervised decision trees for churn prediction, analyzing two methods of hybridization and benchmarking against a non-clustered baseline.)_ |
| **Integrated churn prediction and customer segmentation framework for telco business** | S. Wu, W. C. Yau, T. S. Ong, & S. C. Chong         | Integrated churn prediction and customer segmentation. _(Relevance: Discusses integrating customer segmentation with churn prediction, a core goal of your project.)_                                                                                                                                   |
| **Customer churn prediction system: a machine learning approach**                      | P. Lalwani, M. K. Mishra, J. S. Chadha, & P. Sethi | Customer churn prediction. _(Relevance: Mentions advanced predictive models like Random Forest and Decision Trees for churn prediction.)_                                                                                                                                                               |
| **Deep Learning in Customer Churn Prediction: Unsupervised Feature Learning...**       | Philip Spanoudes, Thomson Nguyen                   | **Churn Prediction, Deep Learning, Neural Networks**. _(Relevance: Compares Random Forest performance against a deep learning pipeline designed to bypass manual feature engineering by learning secondary features in an unsupervised manner.)_                                                        |
