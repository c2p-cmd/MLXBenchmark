# Analysis of Machine Learning Techniques for Segmentation and Prediction

| Focus Area | Specific Questions to Ask NotebookLM | Project Impact | Findings from Sources |
| :--- | :--- | :--- | :--- |
| **Data Preprocessing & Scaling** | **1. "What specific feature scaling techniques (e.g., Standardization vs. Normalization) were applied in the papers using K-Means on financial/credit card data?"** | Helps choose the optimal scikit-learn scaling method before running MLX. | Two main scaling methods were applied to customer data: **Normalization (Min-Max Scaler)** and **Standardization (Z-Score Standardization)**. |
| | | | Specifically concerning *financial/credit card data* (or customer attrition prediction in the banking sector), one study explicitly detailed the preprocessing pipeline: |
| | | | **Standardization** was applied to variables found to have a distribution similar to the normal distribution, namely `Customer_Age` and `Months_on_Book`. Standardization involves centering variables around 0 with a standard deviation of 1. |
| | | | **Normalization** (Min-Max-Scaler) was used for continuous quantitative data that needed transformation, including `Credit_Limit`, `Total_Revolving_Bal`, and `Avg_Open_To_Buy`, as well as `Total_Trans_Amt` and `Total_Trans_Ct`. Normalization is used for unifying data into a defined range, often. |
| | | | Another general discussion notes that prior to applying Gaussian Mixture Model (GMM) clustering, variables are **standardized** to ensure equal importance, though standardization is **not preferred for dummy variables**. |
| **Clustering Parameters (K)** | **2. "In the studies using K-Means on customer data, what were the most commonly reported optimal number of clusters (K) found using the Elbow Method or Silhouette Score?"** | Gives you a strong starting range for K in your own analysis, which is critical for K-Means. | The sources indicate various optimal *K* values based on different datasets and methods: |
| | | | **Elbow Method:** In a study applying K-means clustering to an e-commerce online transaction database using RFM values, the **Elbow Method was utilized to calculate the optimal number of clusters**. |
| | | | **Specific Optimal K Values:** |
| | | | **Banking/Credit Card Data:** For a dataset focused on credit card churn, the Elbow Method yielded an optimal **number of clusters (K) of 6** for K-means clustering. |
| | | | **E-commerce/Retail:** One study using modified dynamic fuzzy *c*-means (dFCM) clustering, which updated segmentation based on RFM patterns, updated the cluster solution from **four initial clusters into five** different segments. Another e-commerce study applying K-means++ categorized customers into **four groups** (iron, copper, silver, and gold). |
| | | | **Telecommunication:** Studies on telecom customer data segmented customers into **4 clusters** based on behavioral patterns and others divided churn customers into **3 clusters** using K-Means. |
| | | | **Domain Expertise:** It is noted that determining *K* is challenging, and in one churn prediction study, **domain experts suggested the number of groups based on their knowledge (i.e., 7 homogeneous groups)**. This suggests 7 is a relevant number to consider in telecommunication contexts. |
| **DBSCAN Parameters** | **3. "In papers that used DBSCAN for customer segmentation, what typical ranges were used for the ϵ (epsilon) and MinSamples parameters on scaled data?"** | Provides guidance for implementing and tuning DBSCAN if you choose that path. | Specific typical ranges for the DBSCAN parameters **$\epsilon$ (epsilon)** and **MinSamples** used in general customer segmentation or on scaled data **were not explicitly detailed** in the provided sources. |
| | | | One source mentioned applying DBSCAN to the UK e-commerce dataset, noting that it can divide datasets into numerous distinct bunches without specifying the optimal number of clusters, and performs well even with noise and outliers present. Another general example, demonstrating the influence of parameters on clustering quality, applied DBSCAN to a 2D synthetic dataset using predefined parameters: **$\epsilon = 0.1$** and **$minPts = 2$**. The general concept is that DBSCAN relies on a distance threshold ($\epsilon$) and the minimum number of points required to form a cluster (MinSamples). |
| **PCA & Feature Reduction** | **4. "Which papers utilized PCA before clustering, and what percentage of variance was captured by the selected principal components?"** | Helps you choose the right number of components for the PCA+K-Means variant of your project. | **PCA (Principal Component Analysis)** is noted as a critical **Dimensionality Reduction** technique used in the preprocessing step for machine learning, especially in clustering algorithms where performance degrades with increasing dimension space. |
| | | | **Utilization of PCA:** |
| | | | **General Workflow:** PCA is mentioned as a feature selection technique for reducing redundant features in a dataset. It is cited as a step used by the `auto ml` AutoML framework when more than 100,000 columns exist, and by `auto-sklearn` for feature engineering. |
| | | | **Before Clustering (Explicitly Mentioned):** PCA is specifically referenced in the context of dimensionality reduction for **telecom customer segmentation**. PCA transforms original variables into a new set of orthogonal components, often serving as a preprocessing step before clustering. |
| | | | **Percentage of Variance Captured:** The sources **do not explicitly state the percentage of variance captured** by the selected principal components in the referenced papers (e.g., in the telecom segmentation study). PCA components are typically ranked by the amount of variance they capture. |
| | | | **Alternative Reduction/Visualization:** Another dimensionality reduction technique, **t-SNE**, was used to visualize the structure of high-dimensional datasets in churn prediction. In one case, t-SNE was used to visualize churner/non-churner separation in feature vectors, where subsequent GMM clustering was performed on the resulting 2-dimensional x, y coordinates. |

| Focus Area | Specific Questions to Ask NotebookLM | Project Impact | Findings from Sources |
| :--- | :--- | :--- | :--- |
| **MLX vs. PyTorch Setup** | **5. "In the MLX vs. PyTorch benchmarks, were there any specific differences in data type usage (e.g., float32 vs. float16) that had a large impact on speed or memory?"** | Helps ensure you configure MLX and PyTorch with the optimal data types for fair comparison. | **Data Type Impact (FP16 vs. FP32):** Data type usage, particularly the lack of optimized FP16 support on Apple Silicon, created a significant performance gap compared to NVIDIA GPUs. |
| | | | **FP32 Performance:** When using the FP32 data type, the performance of MLX and PyTorch's Metal Performance Shaders (MPS) backend was generally **comparable** to CUDA. |
| | | | **FP16 Performance on Apple Silicon:** MLX kernels showed limited benefit from using FP16. For Batched Matrix-Matrix Product, MLX only achieved about a **20%–30% speed benefit** using FP16 over FP32. For Vector-Jacobian Product, MLX or MPS achieved approximately **2x acceleration** with FP16 compared to FP32. |
| | | | **FP16 Performance on NVIDIA/CUDA:** NVIDIA GPUs, optimized with features like Tensor Cores, gained much greater acceleration from FP16, achieving performance increases such as **5x–6x acceleration** for Vector-Jacobian Product. This performance disparity means MLX generally **underperformed CUDA** when using FP16. |
| | | | **AMP Support:** At the time of testing, PyTorch's Automatic Mixed Precision (AMP) lacked support for Apple Silicon SoCs, a feature that could otherwise enhance efficiency. Vendors are recommended to expand FP16 support within AMP. |
| **Memory Management** | **6. "What specific challenges or benefits did the papers mention regarding MLX's unified memory when handling large datasets or models?"** | Prepares you for memory management issues and informs the memory efficiency section of your report. | **Benefits of Unified Memory (MLX):** |
| | | | The architecture integrates CPU and GPU memory into a shared pool, which can be as large as **192GB**. This is highly **beneficial for training large-scale Large Language Models (LLMs)** whose memory requirements might surpass the VRAM limits of discrete GPUs. MLX utilizes this unified memory to allow arrays to be operated across supported devices **without the need for data transfer**, eliminating associated overhead. |
| | | | **Challenges/Trade-offs:** |
| | | | The advantage of memory pooling in Apple Silicon may be **offset by lower raw compute throughput** compared to dedicated GPU workstations. |
| | | | A major challenge is the memory management mechanism itself: Apple Silicon's memory consumption (RSS) often shows a **gradual increase** during training, unlike NVIDIA GPUs where memory usage is relatively stable. This memory management approach is a suspected reason for underperformance. |
| | | | System-level factors such as **page faults** contribute to the performance gap. The number of page faults is observed to **continuously increase** during training, indicating the system is constantly attempting to transfer data into memory. When training near memory capacity, **more page faults are triggered** with a stronger growth trend, leading to a "great performance impact and unsteadiness" in the time required for each pass. |
| **Benchmarking Protocol** | **7. "What was the standard number of repetitions (e.g., 10, 50, 100) used in the LLM/DL benchmarks to ensure reliable runtime measurements on Apple Silicon?"** | Helps you design a statistically robust benchmarking protocol for Objective 4. | **LLM/BLAS Kernel Benchmarking Protocol (MLX/CUDA):** |
| | | | To ensure reliable runtime measurements and minimize the impact of "cold launch" latency, the BLAS kernel analysis (which informs the MLX/Apple Silicon performance discussion) used a specific protocol. Researchers first performed **10 iterations of warm-up**. Following the warm-up, the kernel was launched **100 times continuously and repeatedly**, and the **average** of these results was reported. |
| | | | **General DL Framework Benchmarking:** In an earlier comparative study of deep learning frameworks (Caffe, Neon, etc.), timings were reported as the average of **20–1000 iterations**, typically after conducting several warm-up iterations. |
| | | | **AutoML Benchmarking:** For evaluating statistical performance differences among AutoML frameworks, researchers generated a set of **10 random seeds** to fix randomness and ensure that each framework's performance on a dataset was based on an average of 10 samples. |
